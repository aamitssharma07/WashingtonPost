# Washington Post Article Scraper with Comments Extraction

This repository contains two Python scripts designed to scrape article content, including comments, from The Washington Post website. The scripts use `Selenium` and `BeautifulSoup` to automate the process of logging in, navigating to articles, extracting content, and retrieving comments.

## Prerequisites

- **Python 3.6+**
- **Google Chrome browser**
- **ChromeDriver**
- **Required Python packages** (install using pip):
  ```bash
  pip install requests beautifulsoup4 selenium webdriver-manager
  ```
  Overview
  Step 1: Fetch URLs of Articles
  The first script, ScrapWasPostArtclesURL.py, scrapes article URLs from a specified subsection of The Washington Post (e.g., politics, business, world, sports) and saves them into a JSON file. This script requires user credentials for authentication, which are stored in a config.json file.

Step 2: Scrape Article Content and Comments
After fetching the URLs, the second script, ScrapWasPostArticlesContent.py, takes the JSON file generated in the first step and scrapes each article's content, including the headline, subheading, body text, and comments. The extracted data is saved into individual JSON files.

Configuration File Setup
Before running the scripts, you need to create a config.json file to store your login credentials for The Washington Post.

Example config.json:
json
Copy code
{
"email": "your_email@example.com",
"password": "your_password"
}
This file should be in the root directory of your project.

Script Details
ScrapWasPostArtclesURL.py
Key Features:

Logs into The Washington Post website using credentials from config.json.
Fetches URLs of articles published within a specified date range from a selected subsection.
Saves the fetched URLs into a JSON file.
Usage:

Configuration: Ensure your config.json is properly set up with your email and password.
Run the script: This script will fetch the URLs of articles and save them in a JSON file, which will be used in the next step.
ScrapWasPostArticlesContent.py
Key Features:

Takes the JSON file of URLs generated by ScrapWasPostArtclesURL.py.
Extracts the article content (headline, subheading, body) from each URL.
Locates and extracts comments from the embedded comments iframe.
Saves the article content and comments into individual JSON files.
Usage:

Configuration: Ensure your config.json is properly set up with your email and password.
Run the script: This script will read the URLs from the JSON file generated in the first step, scrape the content and comments, and save the data in individual JSON files.
Directory Structure
plaintext
Copy code
.
├── ScrapWasPostArtclesURL.py
├── ScrapWasPostArticlesContent.py
├── config.json # Your configuration file with credentials
├── URL/ # Folder where the first script stores fetched URLs
│ └── WP*<subsection>\_URL*<date_range>.json
└── Data/ # Folder where the second script stores the article content and comments
└── <article_json_files>.json
Steps to Run the Scripts
Step 1: Run ScrapWasPostArtclesURL.py to fetch the article URLs from The Washington Post.
This will create a JSON file containing a list of URLs based on the date range and subsection you specify.
Step 2: Run ScrapWasPostArticlesContent.py to scrape the content and comments from the URLs generated in Step 1.
This script will create individual JSON files for each article, containing the headline, subheading, body, and comments.
Notes
Authentication:

Both scripts require login credentials to access The Washington Post's content.
Ensure you create and configure the config.json file correctly with your email and password.
Subsections:

The first script supports scraping from various subsections like politics, business, world, technology, and more.
Error Handling:

The scripts include error handling for various cases, including missing elements, failed authentication, and more. Invalid URLs or errors during scraping may result in the JSON file being deleted to ensure clean data output.
Customization:

You can easily modify the scripts to scrape different subsections or adjust the date range according to your requirements.
